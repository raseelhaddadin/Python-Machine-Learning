import numpy as np
import pandas as pd

import datetime

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier as DecisionTree
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn import metrics

import seaborn as sns
sns.set(color_codes=True)
import matplotlib
import matplotlib.pyplot as plt

#Loading the data
loan_data = pd.read_csv('./data/Lending_Club_sample_data.csv', encoding='latin-1')
loan_data.head()

#Part I Clean Up the Data
#Dropping columns we don't want & Dropping categorical data columns
# Delete addr_state, emp_title, id, member_id, , emp_length, pymnt_plan, title. so 7 columns 

loan_data.drop(['addr_state','emp_title','id','member_id', 
                'emp_length', 'pymnt_plan','title'], axis = 1, inplace = True)

#Look at the new dataframe with reduced columns
loan_data.head()

#Check for missing values
# Create a boolean dataframe
bool_df = loan_data.isnull()
# Summing the missing values per column
num_miss_per_col = bool_df.sum()
num_miss_per_col

#Drop missing values
loan_data.dropna(inplace = True)
len(loan_data)

#Create dummies for categorical variables
loan_data_with_dummies_auto = pd.get_dummies(loan_data, drop_first = True)
loan_data_with_dummies_auto.head()
list(loan_data_with_dummies_auto)

# We are keeping track of the column that was automatically dropped.
#Find the columns that were dropped
dropped_columns = set(loan_data.columns) - set(loan_data_with_dummies_auto.columns)
added_columns = set(loan_data_with_dummies_auto.columns) - set(loan_data.columns)

print("Columns dropped during one-hot encoding:")
print(list(dropped_columns))

print("\nColumns added during one-hot encoding:")
print(list(added_columns))

#Separate the input characteristics (X) and the outocme variable (Y)
loan_data_with_dummies_auto_X = loan_data_with_dummies_auto.drop('default_ind', axis=1)
loan_data_with_dummies_auto_Y = loan_data_with_dummies_auto['default_ind']
loan_data_X = loan_data.drop('default_ind', axis=1)
loan_data_Y = loan_data['default_ind']

#Part III Split the Data
loan_train_X, loan_test_X, loan_train_Y, loan_test_Y = train_test_split(loan_data_with_dummies_auto_X, loan_data_with_dummies_auto_Y, random_state = 42, train_size = 0.7)

#Part IV Learn the Classifiers
import sklearn as skl
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import sklearn.metrics as sklmetrics
from sklearn.model_selection import GridSearchCV
import warnings
warnings.filterwarnings('ignore')

#Logistic Regression Classifier
# Step 0
model = LogisticRegression(class_weight='balanced', solver = 'liblinear', random_state = 42)
# Step 1: Specify the dictionary of the parameters 
params = {'penalty':['l1','l2'],
          'C':[0.01, 0.1, 1, 10, 100]}

# Step 2: Prepare the GridSearch for cross validation
grid_search_log_reg = GridSearchCV(model, # Note the model is DecisionTreeClassifier as stated above
                                    param_grid=params, # The parameters to search over. 
                                   cv=10, # How many hold out sets to use
                                   n_jobs = 5 # Number of parallel processes to run. 
                                   )

# Step 3: Do the cross validation on the training data 
grid_search_log_reg.fit(loan_train_X, loan_train_Y)
# Step 4: Select the best model
best_log_reg_cv = grid_search_log_reg.best_estimator_
# Step 4.1: Print the best parameter combination 
print(grid_search_log_reg.best_params_) # {'C': 1, 'penalty': 'l1'}

#Decision Tree Classifier
# For decision tree based classification

# The model you want to set the parameters for
model = DecisionTreeClassifier(class_weight='balanced', random_state=42)

# The parameters to search over for the model
params = {'max_depth':[2,3,4],
          'max_features':['auto','log2',None]}
# Prepare the GridSearch for cross validation
grid_search_dec_tree = GridSearchCV(model, # Note the model is DecisionTreeClassifier as stated above
                                    param_grid=params, # The parameters to search over. 
                                   cv=10, # How many hold out sets to use
                                   n_jobs = 1 # Number of parallel processes to run. 
                                   )
# Do the cross validation on the training data 
grid_search_dec_tree.fit(loan_train_X, loan_train_Y)

# Select the best model
best_dec_tree_cv = grid_search_dec_tree.best_estimator_

# Print the best parameter combination 
print(grid_search_dec_tree.best_params_) # 'max_depth': 4, 'max_features': None

#Part V Testing Performance of Classifiers
#Logistic Regression - Testing Performance of the Best Model on Test Data
loan_predict_Y = best_log_reg_cv.predict(loan_test_X)
print("The accuracy is {0}".format(sklmetrics.accuracy_score(loan_test_Y, loan_predict_Y)))

conf_mat = sklmetrics.confusion_matrix(loan_test_Y, loan_predict_Y, labels =[0,1])
print(conf_mat)

sns.heatmap(conf_mat, square=True, annot=True, cbar = False, xticklabels = ['Not Defaulted','Defaulted'], 
            yticklabels = ['Not Defaulted','Defaulted'], fmt = 'g')
plt.xlabel("Predicted Value")
plt.ylabel("True Value")

#Decision Tree - Testing Performance of the Best Model on Test Data
loan_predict_Y = best_dec_tree_cv.predict(loan_test_X)
print("The accuracy is {0}".format(sklmetrics.accuracy_score(loan_test_Y, loan_predict_Y)))
conf_mat = sklmetrics.confusion_matrix(loan_test_Y, loan_predict_Y, labels =[0,1])
print(conf_mat)

sns.heatmap(conf_mat, square=True, annot=True, cbar = False, xticklabels = ['Not Defaulted','Defaulted'], 
            yticklabels = ['Not Defaulted','Defaulted'], fmt = 'g')
plt.xlabel("Predicted Value")
plt.ylabel("True Value")
#cross validation, confusion matrix and then feature importance. 

#Logistic Regression - Plotting feature importance coefficients for the variables
# Defining a function to plot coefficients as feature importance
# INPUT: Used for Logistic Regression Classifier
#        Feature Names
# OUTPUT: A plot of top most Coefficients
def plot_feature_importance_coeff(model, Xnames, cls_nm = None):

    imp_features = pd.DataFrame(np.column_stack((Xnames, model.coef_.ravel())), columns = ['feature', 'importance'])
    imp_features[['importance']] = imp_features[['importance']].astype(float)
    imp_features[['abs_importance']] = imp_features[['importance']].abs()
    # Sort the features based on absolute value of importance
    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])
    
    # Plot the feature importances 
    plt.figure(figsize=(10, 16))
    plt.title(cls_nm + " - Feature Importance")
    plt.barh(range(imp_features.shape[0]), imp_features['importance'],
            color="b", align="center")
    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )
    plt.ylim([-1, imp_features.shape[0]])
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.tight_layout() 
    plt.savefig(cls_nm + "_feature_imp.png", bbox_inches='tight')
    plt.show()

plot_feature_importance_coeff(best_log_reg_cv, loan_train_X.columns, cls_nm="Logistic Regression")

#Decision Tree - Plotting feature importance coefficients for the variablesÂ¶
# Defining a function to plot feature importance for trees
# INPUT: Used for Tree based Classifier
#        Feature Names
# OUTPUT: A plot of top most features

def plot_feature_importance(model, Xnames, cls_nm = None):

    # Measuring important features
    imp_features = pd.DataFrame(np.column_stack((Xnames, model.feature_importances_)), columns = ['feature', 'importance'])
    imp_features[['importance']] = imp_features[['importance']].astype(float)
    imp_features[['abs_importance']] = imp_features[['importance']].abs()
    # Sort the features based on absolute value of importance
    imp_features = imp_features.sort_values(by = ['abs_importance'], ascending = [1])
    
    # Plot the feature importances
    plt.figure(figsize=(10, 16))
    plt.title(cls_nm + " - Feature Importance")
    plt.barh(range(imp_features.shape[0]), imp_features['importance'],
            color="b", align="center")
    plt.yticks(range(imp_features.shape[0]), imp_features['feature'], )
    plt.ylim([-1, imp_features.shape[0]])
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.tight_layout() 
    plt.savefig(cls_nm + "_feature_imp.png", bbox_inches='tight')
    plt.show()

plot_feature_importance(best_dec_tree_cv, loan_train_X.columns, cls_nm='Decision Tree Classifier')


